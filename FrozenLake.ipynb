{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FrozenLake.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPI0pmLXzG71fDnwAYtfwDA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zanzivyr/Optimizers/blob/main/FrozenLake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning\n",
        "\n",
        "**Frozen Lake** \n",
        "\n",
        "I'm following a video series from DeepLizard. The code is a tutorial from user simoninithomas. The purpose of this study is to get a hands-on, practical, experience with RL. This example clearly shows how to move from the mathematical concepts of MDP's (Markov Desicion Processes) and Q-Learning, to real-world results.\n",
        "\n",
        "- DeepLizard, RL Playlist - https://www.youtube.com/playlist?list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv\n",
        "- simoninithomas, Deep RL Frozen Lake Example - https://github.com/simoninithomas/Deep_reinforcement_learning_Course/tree/master/Q%20learning/FrozenLake"
      ],
      "metadata": {
        "id": "7iwS7L5hNn1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random"
      ],
      "metadata": {
        "id": "_IE66_YCQXDP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install OpenGym"
      ],
      "metadata": {
        "id": "6hBoJfiAajxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v0\")"
      ],
      "metadata": {
        "id": "ajRICDKZQlhK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_size = env.action_space.n\n",
        "state_size = env.observation_space.n\n",
        "\n",
        "action_size, state_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsKPh1nCQpPB",
        "outputId": "3520f625-bafc-4f21-b1f9-b75f7a91dfa9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 16)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we setup the q table where we will write all of our state-action pairs."
      ],
      "metadata": {
        "id": "SbFsmzqRayLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_table = np.zeros((state_size, action_size))\n",
        "print(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAVxcMXcQv_K",
        "outputId": "121f0aef-3c9e-4db5-c910-8797f4c4d537"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "NA7Kvka2ae4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 15000              # Total episodes\n",
        "learning_rate = 0.7               # Learning Rate\n",
        "max_steps_per_episode = 99        # Max steps per episode\n",
        "discount_rate = 0.65              # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "exploration_rate = 1.0            # Exploration rate\n",
        "max_exploration_rate = 1.0        # Exploration probability at start\n",
        "min_exploration_rate = 0.01       # Minimum exploration probability\n",
        "exploration_decay_rate = 0.005    # Exponential decay"
      ],
      "metadata": {
        "id": "outvMBrUQ4R5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Learning\n",
        "\n",
        "1. Initialize Q-values (state-action pair) arbitrarily\n",
        "2. For each episode:\n",
        "  3. Choose an action (a) in the current state (s), based on current Q-value estimate\n",
        "  4. Take the action and observe the outcome state and reward (s' and r)\n",
        "  5. Update bellman optimality equation\n",
        "\n",
        "**Bellman Optimality Equation**\n",
        "\n",
        "q_star(s,a) = E[ r_t+1 + gamma * max(a') q_star(s',a') ]\n",
        "\n",
        "where,\n",
        "\n",
        "- r is reward\n",
        "- gamma is discount rate\n",
        "- s' next state\n",
        "- a' next action"
      ],
      "metadata": {
        "id": "ACpHMrZUa7MR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xlMab8X4vVET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "689c15f8-688b-4753-cb4b-d13d5ec020b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******Average reward per thousand episodes******\n",
            "\n",
            "1000 :,  0.06700000000000005\n",
            "2000 :,  0.25400000000000017\n",
            "3000 :,  0.22100000000000017\n",
            "4000 :,  0.21800000000000017\n",
            "5000 :,  0.19500000000000015\n",
            "6000 :,  0.21000000000000016\n",
            "7000 :,  0.17800000000000013\n",
            "8000 :,  0.2810000000000002\n",
            "9000 :,  0.23900000000000018\n",
            "10000 :,  0.20800000000000016\n",
            "11000 :,  0.2480000000000002\n",
            "12000 :,  0.23200000000000018\n",
            "13000 :,  0.23200000000000018\n",
            "14000 :,  0.21500000000000016\n",
            "15000 :,  0.18100000000000013\n",
            "\n",
            "\n",
            "*********Q-table***********\n",
            "\n",
            "[[1.70364197e-07 7.94478596e-05 1.55358703e-05 1.75294310e-05]\n",
            " [5.04838558e-06 4.08294329e-06 6.25912311e-08 1.08134303e-04]\n",
            " [6.46750077e-04 1.99965905e-06 1.03089789e-06 1.61780972e-06]\n",
            " [1.42444400e-07 2.04799899e-07 7.57253141e-08 2.26747543e-07]\n",
            " [2.74669837e-05 2.99196350e-05 2.98042624e-05 2.91717735e-09]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.67702020e-02 1.35000909e-07 1.00062214e-06 3.90226047e-07]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [9.90589109e-07 8.20569546e-04 7.63169485e-07 7.03582099e-05]\n",
            " [1.90569545e-04 5.73455438e-03 8.91842504e-05 2.37289403e-04]\n",
            " [3.29590470e-01 2.72940543e-05 1.22852757e-03 1.14946593e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.40442695e-04 1.39725271e-04 5.72868766e-04 1.54614045e-03]\n",
            " [7.35778478e-03 4.62394813e-03 9.15025431e-01 4.54303056e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ]
        }
      ],
      "source": [
        "rewards_all_episodes = []\n",
        "\n",
        "# Q-Learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "  state = env.reset() # resets state back to default each episode\n",
        "\n",
        "  done = False\n",
        "  rewards_current_episode = 0\n",
        "\n",
        "  for step in range(max_steps_per_episode):\n",
        "\n",
        "    # Exploration-exploitation trade-off\n",
        "    exploration_rate_threshold = random.uniform(0, 1)\n",
        "    if exploration_rate_threshold > exploration_rate:\n",
        "      action = np.argmax(q_table[state,:])\n",
        "    else:\n",
        "      action = env.action_space.sample()\n",
        "\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "\n",
        "    # Update Q-table for Q(s,a)\n",
        "    q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
        "      learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
        "\n",
        "    state = new_state\n",
        "    rewards_current_episode += reward\n",
        "\n",
        "    if done == True:\n",
        "      break\n",
        "\n",
        "  # Exploration rate decay\n",
        "  exploration_rate = min_exploration_rate + \\\n",
        "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
        "\n",
        "  rewards_all_episodes.append(rewards_current_episode)\n",
        "\n",
        "# After all episodes are finished,\n",
        "# Calculate and print the average reward per thousand episodes\n",
        "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)\n",
        "count = 1000\n",
        "print(\"******Average reward per thousand episodes******\\n\")\n",
        "for r in rewards_per_thousand_episodes:\n",
        "  print(count, \":, \", str(sum(r/1000)))\n",
        "  count += 1000\n",
        "\n",
        "# Print updated Q-table\n",
        "print(\"\\n\\n*********Q-table***********\\n\")\n",
        "print(q_table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Play Frozen Lake\n",
        "\n",
        "Now that our q_table is trained, we can use it to play Frozen Lake."
      ],
      "metadata": {
        "id": "XRLiJjypbCeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "\n",
        "for episode in range(5):\n",
        "  state = env.reset()\n",
        "  step = 0\n",
        "  done = False\n",
        "  print(\"==================\")\n",
        "  print(\"EPISODE \", episode)\n",
        "\n",
        "  for step in range(max_steps_per_episode):\n",
        "\n",
        "    # Take the action (index that have the maximum expected future reward given that state)\n",
        "    action = np.argmax(q_table[state,:])\n",
        "\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "\n",
        "    if done:\n",
        "      # Here, we decide to only print the last state (to see if our agent is on goal or falls into a hole)\n",
        "      env.render()\n",
        "\n",
        "      # We print the number of steps it took\n",
        "      print(\"Number of steps\", step)\n",
        "      break\n",
        "\n",
        "    state = new_state\n",
        "\n",
        "  env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4AiXRu2RiKl",
        "outputId": "dc14b469-a41d-4954-ebb3-352e8d2d61a9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================\n",
            "EPISODE  0\n",
            "  (Down)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of steps 2\n",
            "==================\n",
            "EPISODE  1\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Number of steps 4\n",
            "==================\n",
            "EPISODE  2\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of steps 24\n",
            "==================\n",
            "EPISODE  3\n",
            "  (Down)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of steps 4\n",
            "==================\n",
            "EPISODE  4\n",
            "  (Down)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of steps 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BFd-vZl2Z_Jg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}